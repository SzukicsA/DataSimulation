score = rnorm(n = n_intervention, mean = mean_intervention, sd = sd_intervention))
) |>
# control's factor levels must be ordered so that intervention is the first level and control is the second
# this ensures that positive cohen's d values refer to intervention > control and not the other way around.
mutate(condition = fct_relevel(condition, "intervention", "control"))
return(data)
}
# define data analysis function ----
analyse_data <- function(data) {
res_t_test <- t.test(formula = score ~ condition,
data = data,
var.equal = FALSE,
alternative = "two.sided")
res <- tibble(p = res_t_test$p.value)
return(res)
}
# define experiment parameters ----
experiment_parameters_grid <- expand_grid(
n_control = 50,
n_intervention = 50,
mean_control = 0,
mean_intervention = c(0.0, 0.1, 0.2, 0.5, 0.8, 1.0), # only this differs meaningfully from the simulation in lesson 2: simulate data for a true effect size of 0 (null) and very small, small, medium, large, and very large Cohen's d (alternative)
sd_control = 1,
sd_intervention = 1,
iteration = 1:10000
)
# run simulation ----
simulation <-
# using the experiment parameters
experiment_parameters_grid |>
# generate data using the data generating function and the parameters relevant to data generation
mutate(generated_data = pmap(list(n_control,
n_intervention,
mean_control,
mean_intervention,
sd_control,
sd_intervention),
generate_data)) |>
# apply the analysis function to the generated data using the parameters relevant to analysis
mutate(analysis_results = pmap(list(generated_data),
analyse_data))
# summarise simulation results over the iterations ----
simulation_reshaped <- simulation |>
# convert `analysis_results` nested-data-frame column to regular columns in the df. in this case, the p value.
unnest(analysis_results) |>
# label the true effect value
mutate(true_effect = paste("Cohen's d =", mean_intervention))
simulation_reshaped |>
# using only the iterations where the population effect was from a true null population
filter(true_effect == "Cohen's d = 0") |>
# plot using our function
plot_p_values()
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0") |>
summarize(false_positive_rate__aka_alpha = mean(p < .05)) |>
mutate_if(is.numeric, janitor::round_half_up, digits = 2)
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0.1") |>
plot_p_values()
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0.2") |>
plot_p_values()
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0.5") |>
plot_p_values()
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0.8") |>
plot_p_values()
simulation_reshaped |>
filter(true_effect == "Cohen's d = 1") |>
plot_p_values()
simulation_reshaped |>
filter(true_effect != "Cohen's d = 0") |>
summarize(true_positive_rate__aka_power = mean(p < .05),
.by = true_effect) |>
mutate_if(is.numeric, janitor::round_half_up, digits = 2)
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0") |>
filter(iteration < 150) |>
mutate(decision = ifelse(p < .05, "significant", "non-significant")) |>
ggplot(aes(iteration, p)) +
geom_line(color = "darkgrey") +
geom_point(aes(color = decision)) +
scale_color_viridis_d(option = "mako", begin = 0.3, end = 0.7, direction = -1) +
theme_linedraw() +
geom_hline(yintercept = 0.05, linetype = "dashed") +
xlab("Experiment (iteration)")
k_null <- runif(n = 1, min = 0, max = 100) |> round(0)
k_non_null <- 100 - k_null
# simulate published literature by sampling from the existing simulation iterations
unbiased_literature <-
bind_rows(
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0") |>
sample_n(k_null),
simulation_reshaped |>
filter(true_effect %in% c("Cohen's d = 0.1", "Cohen's d = 0.2", "Cohen's d = 0.5")) |>
sample_n(k_non_null)
) |>
mutate(truth_vs_decision = case_when(true_effect == "Cohen's d = 0" & p >= .05 ~ "True negatives",
true_effect == "Cohen's d = 0" & p <  .05 ~ "False positives",
true_effect != "Cohen's d = 0" & p >= .05 ~ "False negatives",
true_effect != "Cohen's d = 0" & p <  .05 ~ "True positives"))
unbiased_literature |>
plot_p_values()
unbiased_literature |>
ggplot(aes(p, fill = truth_vs_decision)) +
geom_histogram(binwidth = 0.05, boundary = 0) +
scale_fill_viridis_d(option = "magma", begin = 0.2, end = 0.9, direction = -1) +
scale_x_continuous(labels = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),
breaks = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),
limits = c(0, 1)) +
theme_linedraw() +
ylab("Frequency")
unbiased_literature |>
count(truth_vs_decision) |>
pivot_wider(names_from = truth_vs_decision,
values_from = n) |>
mutate(`False discovery rate` = round_half_up(`False positives` / (`False positives` + `True positives`), digits = 2),
`Missed discovery rate` = round_half_up(`False negatives` / (`False negatives` + `True negatives`), digits = 2)) |>
#pivot_longer(cols = everything()) |>
#mutate_if(is.numeric, janitor::round_half_up, digits = 2) |>
kable() |>
kable_classic(full_width = FALSE)
library(kableExtra)
# remove all objects from environment ----
#rm(list = ls())
# dependencies ----
# repeated here for the sake of completeness
library(tidyr)
library(dplyr)
library(forcats)
library(readr)
library(purrr)
library(ggplot2)
library(effsize)
library(kableExtra)
# set the seed ----
# for the pseudo random number generator to make results reproducible
set.seed(123)
# define data generating function ----
generate_data <- function(n_control,
n_intervention,
mean_control,
mean_intervention,
sd_control,
sd_intervention) {
data <-
bind_rows(
tibble(condition = "control",
score = rnorm(n = n_control, mean = mean_control, sd = sd_control)),
tibble(condition = "intervention",
score = rnorm(n = n_intervention, mean = mean_intervention, sd = sd_intervention))
) |>
# control's factor levels must be ordered so that intervention is the first level and control is the second
# this ensures that positive cohen's d values refer to intervention > control and not the other way around.
mutate(condition = fct_relevel(condition, "intervention", "control"))
return(data)
}
# define data analysis function ----
analyse_data <- function(data) {
res_t_test <- t.test(formula = score ~ condition,
data = data,
var.equal = FALSE,
alternative = "two.sided")
res <- tibble(p = res_t_test$p.value)
return(res)
}
# define experiment parameters ----
experiment_parameters_grid <- expand_grid(
n_control = 50,
n_intervention = 50,
mean_control = 0,
mean_intervention = c(0.0, 0.1, 0.2, 0.5, 0.8, 1.0), # only this differs meaningfully from the simulation in lesson 2: simulate data for a true effect size of 0 (null) and very small, small, medium, large, and very large Cohen's d (alternative)
sd_control = 1,
sd_intervention = 1,
iteration = 1:10000
)
# run simulation ----
simulation <-
# using the experiment parameters
experiment_parameters_grid |>
# generate data using the data generating function and the parameters relevant to data generation
mutate(generated_data = pmap(list(n_control,
n_intervention,
mean_control,
mean_intervention,
sd_control,
sd_intervention),
generate_data)) |>
# apply the analysis function to the generated data using the parameters relevant to analysis
mutate(analysis_results = pmap(list(generated_data),
analyse_data))
# summarise simulation results over the iterations ----
simulation_reshaped <- simulation |>
# convert `analysis_results` nested-data-frame column to regular columns in the df. in this case, the p value.
unnest(analysis_results) |>
# label the true effect value
mutate(true_effect = paste("Cohen's d =", mean_intervention))
k_null <- runif(n = 1, min = 0, max = 100) |> round(0)
k_non_null <- 100 - k_null
# simulate published literature by sampling from the existing simulation iterations
unbiased_literature <-
bind_rows(
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0") |>
sample_n(k_null),
simulation_reshaped |>
filter(true_effect %in% c("Cohen's d = 0.1", "Cohen's d = 0.2", "Cohen's d = 0.5")) |>
sample_n(k_non_null)
) |>
mutate(truth_vs_decision = case_when(true_effect == "Cohen's d = 0" & p >= .05 ~ "True negatives",
true_effect == "Cohen's d = 0" & p <  .05 ~ "False positives",
true_effect != "Cohen's d = 0" & p >= .05 ~ "False negatives",
true_effect != "Cohen's d = 0" & p <  .05 ~ "True positives"))
unbiased_literature |>
ggplot(aes(p, fill = truth_vs_decision)) +
geom_histogram(binwidth = 0.05, boundary = 0) +
scale_fill_viridis_d(option = "magma", begin = 0.2, end = 0.9, direction = -1) +
scale_x_continuous(labels = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),
breaks = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),
limits = c(0, 1)) +
theme_linedraw() +
ylab("Frequency")
unbiased_literature |>
count(truth_vs_decision) |>
pivot_wider(names_from = truth_vs_decision,
values_from = n) |>
mutate(`False discovery rate` = round_half_up(`False positives` / (`False positives` + `True positives`), digits = 2),
`Missed discovery rate` = round_half_up(`False negatives` / (`False negatives` + `True negatives`), digits = 2)) |>
#pivot_longer(cols = everything()) |>
#mutate_if(is.numeric, janitor::round_half_up, digits = 2) |>
kable() |>
kable_classic(full_width = FALSE)
library(tidyverse)
# sample values from a uniform distribution, from the range 0 to 1
tibble(p = runif(n = 100000, min = 0, max = 1)) |>
# create a decision for each value, with values < .05 labelled as "significant" and those > .05 as "non-significant"
mutate(decision = ifelse(p < .05, "significant", "non-significant")) |>
# plot a histogram of these values, with the fill contingent on the the decision
ggplot(aes(p, fill = decision)) +
geom_histogram(binwidth = 0.05, boundary = 0) +
scale_fill_viridis_d(option = "mako", begin = 0.3, end = 0.7, direction = -1) +
scale_x_continuous(labels = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),
breaks = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),
limits = c(0, 1)) +
theme_linedraw() +
ylab("Frequency")
# sample values from a uniform distribution, from the range 0 to 1
tibble(p = rnorm(n = 100000, mean = 0.5, sd = 0.25)) |>
# drop all values that are outside the range [0, 1] to mimic p values
filter(p >= 0 & p <= 1) |>
# create a decision for each value, with values < .05 labelled as "significant" and those > .05 as "non-significant"
mutate(decision = ifelse(p < .05, "significant", "non-significant")) |>
# plot a histogram of these values, with the fill contingent on the the decision
ggplot(aes(p, fill = decision)) +
geom_histogram(binwidth = 0.05, boundary = 0) +
scale_fill_viridis_d(option = "mako", begin = 0.3, end = 0.7, direction = -1) +
scale_x_continuous(labels = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),
breaks = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),
limits = c(0, 1)) +
theme_linedraw() +
ylab("Frequency")
plot_p_values <- function(data){ # assumes that data is a data frame with a column "p"
data |>
mutate(decision = ifelse(p < .05, "significant", "non-significant")) |>
ggplot(aes(p, fill = decision)) +
geom_histogram(binwidth = 0.05, boundary = 0) +
scale_fill_viridis_d(option = "mako", begin = 0.3, end = 0.7, direction = -1) +
scale_x_continuous(labels = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),
breaks = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),
limits = c(0, 1)) +
theme_linedraw() +
ylab("Frequency")
}
tibble(p = rbeta(n = 100000, shape1 = 5, shape2 = 5)) |>
plot_p_values()
tibble(p = rbeta(n = 100000, shape1 = 1, shape2 = 1)) |>
plot_p_values()
tibble(p = rbeta(n = 100000, shape1 = 1, shape2 = 5)) |>
plot_p_values()
tibble(p = rbeta(n = 100000, shape1 = 0.1, shape2 = 5)) |>
plot_p_values()
# remove all objects from environment ----
#rm(list = ls())
# dependencies ----
# repeated here for the sake of completeness
library(tidyr)
library(dplyr)
library(forcats)
library(readr)
library(purrr)
library(ggplot2)
library(effsize)
library(kableExtra)
# set the seed ----
# for the pseudo random number generator to make results reproducible
set.seed(123)
# define data generating function ----
generate_data <- function(n_control,
n_intervention,
mean_control,
mean_intervention,
sd_control,
sd_intervention) {
data <-
bind_rows(
tibble(condition = "control",
score = rnorm(n = n_control, mean = mean_control, sd = sd_control)),
tibble(condition = "intervention",
score = rnorm(n = n_intervention, mean = mean_intervention, sd = sd_intervention))
) |>
# control's factor levels must be ordered so that intervention is the first level and control is the second
# this ensures that positive cohen's d values refer to intervention > control and not the other way around.
mutate(condition = fct_relevel(condition, "intervention", "control"))
return(data)
}
# define data analysis function ----
analyse_data <- function(data) {
res_t_test <- t.test(formula = score ~ condition,
data = data,
var.equal = FALSE,
alternative = "two.sided")
res <- tibble(p = res_t_test$p.value)
return(res)
}
# define experiment parameters ----
experiment_parameters_grid <- expand_grid(
n_control = 50,
n_intervention = 50,
mean_control = 0,
mean_intervention = c(0.0, 0.1, 0.2, 0.5, 0.8, 1.0), # only this differs meaningfully from the simulation in lesson 2: simulate data for a true effect size of 0 (null) and very small, small, medium, large, and very large Cohen's d (alternative)
sd_control = 1,
sd_intervention = 1,
iteration = 1:10000
)
# run simulation ----
simulation <-
# using the experiment parameters
experiment_parameters_grid |>
# generate data using the data generating function and the parameters relevant to data generation
mutate(generated_data = pmap(list(n_control,
n_intervention,
mean_control,
mean_intervention,
sd_control,
sd_intervention),
generate_data)) |>
# apply the analysis function to the generated data using the parameters relevant to analysis
mutate(analysis_results = pmap(list(generated_data),
analyse_data))
# summarise simulation results over the iterations ----
simulation_reshaped <- simulation |>
# convert `analysis_results` nested-data-frame column to regular columns in the df. in this case, the p value.
unnest(analysis_results) |>
# label the true effect value
mutate(true_effect = paste("Cohen's d =", mean_intervention))
simulation_reshaped |>
# using only the iterations where the population effect was from a true null population
filter(true_effect == "Cohen's d = 0") |>
# plot using our function
plot_p_values()
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0") |>
summarize(false_positive_rate__aka_alpha = mean(p < .05)) |>
mutate_if(is.numeric, janitor::round_half_up, digits = 2)
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0.1") |>
plot_p_values()
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0.2") |>
plot_p_values()
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0.5") |>
plot_p_values()
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0.8") |>
plot_p_values()
simulation_reshaped |>
filter(true_effect == "Cohen's d = 1") |>
plot_p_values()
simulation_reshaped |>
filter(true_effect != "Cohen's d = 0") |>
summarize(true_positive_rate__aka_power = mean(p < .05),
.by = true_effect) |>
mutate_if(is.numeric, janitor::round_half_up, digits = 2)
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0") |>
filter(iteration < 150) |>
mutate(decision = ifelse(p < .05, "significant", "non-significant")) |>
ggplot(aes(iteration, p)) +
geom_line(color = "darkgrey") +
geom_point(aes(color = decision)) +
scale_color_viridis_d(option = "mako", begin = 0.3, end = 0.7, direction = -1) +
theme_linedraw() +
geom_hline(yintercept = 0.05, linetype = "dashed") +
xlab("Experiment (iteration)")
k_null <- runif(n = 1, min = 0, max = 100) |> round(0)
k_non_null <- 100 - k_null
# simulate published literature by sampling from the existing simulation iterations
unbiased_literature <-
bind_rows(
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0") |>
sample_n(k_null),
simulation_reshaped |>
filter(true_effect %in% c("Cohen's d = 0.1", "Cohen's d = 0.2", "Cohen's d = 0.5")) |>
sample_n(k_non_null)
) |>
mutate(truth_vs_decision = case_when(true_effect == "Cohen's d = 0" & p >= .05 ~ "True negatives",
true_effect == "Cohen's d = 0" & p <  .05 ~ "False positives",
true_effect != "Cohen's d = 0" & p >= .05 ~ "False negatives",
true_effect != "Cohen's d = 0" & p <  .05 ~ "True positives"))
unbiased_literature |>
plot_p_values()
unbiased_literature |>
ggplot(aes(p, fill = truth_vs_decision)) +
geom_histogram(binwidth = 0.05, boundary = 0) +
scale_fill_viridis_d(option = "magma", begin = 0.2, end = 0.9, direction = -1) +
scale_x_continuous(labels = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),
breaks = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),
limits = c(0, 1)) +
theme_linedraw() +
ylab("Frequency")
unbiased_literature |>
count(truth_vs_decision) |>
pivot_wider(names_from = truth_vs_decision,
values_from = n) |>
mutate(`False discovery rate` = round_half_up(`False positives` / (`False positives` + `True positives`), digits = 2),
`Missed discovery rate` = round_half_up(`False negatives` / (`False negatives` + `True negatives`), digits = 2)) |>
#pivot_longer(cols = everything()) |>
#mutate_if(is.numeric, janitor::round_half_up, digits = 2) |>
kable() |>
kable_classic(full_width = FALSE)
library(janitor)
simulation_reshaped |>
# using only the iterations where the population effect was from a true null population
filter(true_effect == "Cohen's d = 0") |>
# plot using our function
plot_p_values()
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0") |>
summarize(false_positive_rate__aka_alpha = mean(p < .05)) |>
mutate_if(is.numeric, janitor::round_half_up, digits = 2)
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0.1") |>
plot_p_values()
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0.2") |>
plot_p_values()
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0.5") |>
plot_p_values()
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0.8") |>
plot_p_values()
simulation_reshaped |>
filter(true_effect == "Cohen's d = 1") |>
plot_p_values()
simulation_reshaped |>
filter(true_effect != "Cohen's d = 0") |>
summarize(true_positive_rate__aka_power = mean(p < .05),
.by = true_effect) |>
mutate_if(is.numeric, janitor::round_half_up, digits = 2)
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0") |>
filter(iteration < 150) |>
mutate(decision = ifelse(p < .05, "significant", "non-significant")) |>
ggplot(aes(iteration, p)) +
geom_line(color = "darkgrey") +
geom_point(aes(color = decision)) +
scale_color_viridis_d(option = "mako", begin = 0.3, end = 0.7, direction = -1) +
theme_linedraw() +
geom_hline(yintercept = 0.05, linetype = "dashed") +
xlab("Experiment (iteration)")
k_null <- runif(n = 1, min = 0, max = 100) |> round(0)
k_non_null <- 100 - k_null
# simulate published literature by sampling from the existing simulation iterations
unbiased_literature <-
bind_rows(
simulation_reshaped |>
filter(true_effect == "Cohen's d = 0") |>
sample_n(k_null),
simulation_reshaped |>
filter(true_effect %in% c("Cohen's d = 0.1", "Cohen's d = 0.2", "Cohen's d = 0.5")) |>
sample_n(k_non_null)
) |>
mutate(truth_vs_decision = case_when(true_effect == "Cohen's d = 0" & p >= .05 ~ "True negatives",
true_effect == "Cohen's d = 0" & p <  .05 ~ "False positives",
true_effect != "Cohen's d = 0" & p >= .05 ~ "False negatives",
true_effect != "Cohen's d = 0" & p <  .05 ~ "True positives"))
unbiased_literature |>
plot_p_values()
unbiased_literature |>
ggplot(aes(p, fill = truth_vs_decision)) +
geom_histogram(binwidth = 0.05, boundary = 0) +
scale_fill_viridis_d(option = "magma", begin = 0.2, end = 0.9, direction = -1) +
scale_x_continuous(labels = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),
breaks = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),
limits = c(0, 1)) +
theme_linedraw() +
ylab("Frequency")
unbiased_literature |>
count(truth_vs_decision) |>
pivot_wider(names_from = truth_vs_decision,
values_from = n) |>
mutate(`False discovery rate` = round_half_up(`False positives` / (`False positives` + `True positives`), digits = 2),
`Missed discovery rate` = round_half_up(`False negatives` / (`False negatives` + `True negatives`), digits = 2)) |>
#pivot_longer(cols = everything()) |>
#mutate_if(is.numeric, janitor::round_half_up, digits = 2) |>
kable() |>
kable_classic(full_width = FALSE)
