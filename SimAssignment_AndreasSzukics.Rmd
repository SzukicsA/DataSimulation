---
title: "Simulation assignment - Soundness of masterâ€™s thesis "
author: "Andreas Szukics"
date: "2024-08-08"
output: html_document
---

# dependencies

```{r}
## Libraries ##
library(tidyr)
library(dplyr)
library(purrr) 
library(ggplot2)
library(ggtext)
library(sn)
library(knitr)
library(kableExtra)
library(janitor)
library(parallel)
library(doParallel)

## Knitr settings ##
knitr::opts_chunk$set(
                      message = FALSE,
                      warning = FALSE
                      )
```


```{r}
## Seed ##
set.seed(42) # the hitchhiker's guide to the galaxy

## Data generation function ##
generate_data <- function(
                          n_control,            # Control
                          n_intervention_1,     # Perceptual learning
                          n_intervention_2,     # Tai-Chi
                          mean_control,
                          mean_intervention_1,
                          mean_intervention_2,
                          sd
                          ) {
  
  data_control <- tibble(
                         condition = "control",
                         score = rnorm(
                                       n = n_control,
                                       mean = mean_control,
                                       sd = sd
                                       )
                         )
  
  data_intervention_1 <- tibble(
                                condition = "PL",
                                score = rnorm(
                                              n = n_intervention_1,
                                              mean = mean_intervention_1,
                                              sd = sd
                                              )
                                )
  
  data_intervention_2 <- tibble(
                                condition = "tai_chi",
                                score = rnorm(
                                              n = n_intervention_2,
                                              mean = mean_intervention_2,
                                              sd = sd
                                              )
                                )
  
  data <- bind_rows(
                    data_control,
                    data_intervention_1,
                    data_intervention_2
                    )
  
  return(data)
  }

## Analysis function ##
analyse_data <- function(data) {
  result_aov <- aov(
                    score ~ condition,
                    data = data
                    )
  p_value <- summary(result_aov)[[1]][["Pr(>F)"]][1]
  tibble(p = p_value)
  }

## Define experiment parameters ##
experiment_parameters_grid <- expand_grid(
                                          n_control = 30,
                                          n_intervention_1 = 30,
                                          n_intervention_2 = 30,
                                          mean_control = 0,          # No true effect (all means are the same)
                                          mean_intervention_1 = 0,   # No true effect (all means are the same)
                                          mean_intervention_2 = 0,   # No true effect (all means are the same)
                                          sd = 1,
                                          iteration = 1:10000
                                          )

## Parallel setup ##
num_cores <- detectCores() - 1  # Use one less than the number of available cores
cl <- makeCluster(
                  num_cores
                  )

# Export necessary objects and functions to the cluster
clusterExport(
              cl,
              list(
                   "experiment_parameters_grid",
                   "generate_data",
                   "analyse_data"
                   )
              )

clusterEvalQ(
             cl,
             {
              library(dplyr)
              library(tidyr)
               }
             )

## Measure the time for the simulation ##
simulation_time <- system.time({
  ## Run simulation with parallel processing ##
  simulation <- experiment_parameters_grid|>
    mutate(
           generated_data = parLapply(
                                      cl,
                                      1:n(),
                                      function(i) {
                                                   set.seed(42 + i)  # Set a different seed for each worker
                                                   params <- experiment_parameters_grid[i, ]
                                                   generate_data(
                                                                 params$n_control,
                                                                 params$n_intervention_1, 
                                                                 params$n_intervention_2,
                                                                 params$mean_control,
                                                                 params$mean_intervention_1,
                                                                 params$mean_intervention_2,
                                                                 params$sd
                                                                 )
                                                   }
                                      )
           )|>
    mutate(
           analysis_results = parLapply(
                                        cl,
                                        generated_data,
                                        analyse_data
                                        )
           )|>
    mutate(
           p_value = map_dbl(
                             analysis_results,
                             ~ .x$p
                             )
           )
  }
  )

## Stop cluster ##
stopCluster(cl)

## Calculate false positive rate ##
result <- simulation|>
  summarise(
            false_positive_rate = mean(p_value < 0.05)
            )|>
  pull(false_positive_rate)

## Print the false positive rate ##
print(result)

## Print the time taken for the simulation ##
print(simulation_time)

```


## Display results in a table

```{r}
simulation_summary <- simulation |>
  unnest(analysis_results) |>
  group_by(
           n_control,
           n_intervention_1,
           n_intervention_2,
           mean_control,
           mean_intervention_1,
           mean_intervention_2,
           sd
           ) |>
  summarize(
            positive_rate = janitor::round_half_up(
                                                   mean(p < .05),
                                                   digits = 3
                                                   )
            ) |>
  ungroup()

simulation_summary |>
  kable() |>
  kable_classic(full_width = FALSE)

```


## Simplified Table with Key Columns and Rounded Values


```{r}

## Simplified Table with Key Columns and Rounded Values ##
simulation_summary <- simulation |>
  unnest(analysis_results) |>
  group_by(
           n_control,
           n_intervention_1,
           n_intervention_2,
           mean_control,
           mean_intervention_1,
           mean_intervention_2,
           sd
           ) |>
  summarize(
            positive_rate = janitor::round_half_up(
                                                   mean(p < .05),
                                                   digits = 3
                                                   ),
            mean_p_value = janitor::round_half_up(
                                                  mean(p_value),
                                                  digits = 3
                                                  ),
            sd_p_value = janitor::round_half_up(
                                                sd(p_value),
                                                digits = 3
                                                )
            ) |>
  ungroup()

## Display Table ##
simulation_summary |>
  kable() |>
  kable_classic(
                full_width = FALSE,
                html_font = "Arial"
                ) |>
  kable_styling(
                bootstrap_options = c(
                                      "striped",
                                      "hover",
                                      "condensed"
                                      ),
                full_width = F
                )
```

## Display Multiple Rows of Results

```{r}
## Display Multiple Rows of Results ##
simulation_summary <- simulation |>
  unnest(analysis_results) |>
  select(
         n_control,
         n_intervention_1,
         n_intervention_2, 
         mean_control, 
         mean_intervention_1, 
         mean_intervention_2, 
         sd, 
         p_value
         ) |>
  mutate(
         p_value = round(p_value, 3)
         )

## Display Table ##
simulation_summary |>
  head(20) |>
  kable(col.names = c("N Control","N PL","N Tai-Chi", "Mean Control", "Mean PL", "Mean Tai-Chi", "sd", "P-Value")) |>
  kable_classic(
    full_width = FALSE,
    html_font = "Arial"
  ) |>
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F
  )


## Display Multiple Rows of Results ##
simulation_summary1 <- simulation |>
  unnest(analysis_results) |>
  select(n_control, mean_control, sd, p_value) |>
  mutate(
    p_value = round(p_value, 3)
  )

# Display Table with renamed columns
simulation_summary1 |>
  head(20) |>
  kable(col.names = c("N", "Mean", "sd", "P-Value")) |>
  kable_classic(
    full_width = FALSE,
    html_font = "Arial"
  ) |>
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F
  )
  


```

## Calculate Means and Standard Errors

```{r}
## Calculate Means and Standard Errors ##
summary_data <- simulation |>
  unnest(generated_data) |>
  group_by(condition) |>
  summarize(
    mean_score = mean(score),
    sem = sd(score) / sqrt(n())
  )

## Create the Plot ##
ggplot(summary_data, aes(x = condition, y = mean_score)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean_score - sem, ymax = mean_score + sem), width = 0.2) +
  labs(
    title = "Mean Score by Condition with Standard Error",
    x = "Condition",
    y = "Mean Score"
  ) +
  theme_minimal()

summary_data |>
  mutate(
    mean_score = round(mean_score, 3),
    sem = round(sem, 3)
  ) |>
  kable(col.names = c("Condition", "Mean Score", "Standard Error"), align = "c") |>
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    position = "center"
  )

```


## False Positive Rate Analysis

```{r}
## Set the significance level
alpha <- 0.05

## Extract p-values from the simulation
p_values <- simulation$p_value

## Calculate the False Positive Rate
false_positive_rate <- mean(p_values < alpha)

## 1. Bar Plot
bar_data <- tibble(
  Outcome = c("False Positive", "True Negative"),
  Count = c(sum(p_values < alpha), sum(p_values >= alpha))
)

ggplot(bar_data, aes(x = Outcome, y = Count)) +
  geom_bar(stat = "identity", fill = c("red", "lightblue")) +
  labs(
    title = "False Positive Rate",
    x = "Outcome",
    y = "Count"
  ) +
  theme_minimal()

## 2. Line Plot
fpr_data <- tibble(
  Iteration = 1:length(p_values),
  FPR = cumsum(p_values < alpha) / (1:length(p_values))
)

ggplot(fpr_data, aes(x = Iteration, y = FPR)) +
  geom_line(color = "red") +
  labs(
    title = "False Positive Rate Over Iterations",
    x = "Iteration",
    y = "Cumulative False Positive Rate"
  ) +
  theme_minimal()

## 3. Histogram of p-values with FPR line
ggplot(tibble(p_values), aes(x = p_values)) +
  geom_histogram(bins = 50, fill = "lightblue", color = "white") +
  geom_vline(xintercept = alpha, color = "red", linetype = "dashed") +
  scale_x_continuous(labels = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),
                     breaks = c(0, 0.05, 0.25, 0.50, 0.75, 1.0), 
                     limits = c(0, 1)) +
  labs(
    title = "Histogram of p-values",
    x = "p-value",
    y = "Frequency"
  ) +
  theme_minimal()


```


```{r}
## Set parameters for power analysis ##
effect_sizes <- c(
  0.2,
  0.5,
  0.8
)  # Small, medium, large effect sizes
sample_sizes <- seq(
  30,
  90,
  by = 10
)  # Sample sizes to test
alpha <- 0.05  # Significance level
desired_power <- 0.8  # Desired power level
iterations <- 10000  # Number of iterations

## Function to calculate power ##
calculate_power <- function(n, effect_size, iterations, alpha) {
  generate_data <- function(n, mean_diff, sd = 1) {
    control <- rnorm(n, mean = 0, sd = sd)
    treatment <- rnorm(n, mean = mean_diff, sd = sd)
    data <- data.frame(
      group = factor(rep(c("control", "treatment"), each = n)),
      score = c(control, treatment)
    )
    return(data)
  }
  
  results <- foreach(i = 1:iterations, .combine = c) %dopar% {
    data <- generate_data(n, effect_size)
    p_value <- summary(aov(score ~ group, data = data))[[1]][["Pr(>F)"]][1]
    return(p_value < alpha)
  }
  
  power <- mean(results)
  return(power)
}

## Perform power analysis ##
power_results <- expand.grid(
  n = sample_sizes,
  effect_size = effect_sizes
)

## Set up parallel backend ##
cl <- makeCluster(detectCores() - 1)  # Leave one core free
registerDoParallel(cl)

## Measure the time for the power analysis ##
power_analysis_time <- system.time({
  power_results <- power_results %>%
    rowwise() %>%
    mutate(
      power = calculate_power(n, effect_size, iterations, alpha)
    )
})

## Stop the parallel backend ##
stopCluster(cl)
registerDoSEQ()

## Sample size determination for desired power ##
sample_size_for_power <- power_results %>%
  filter(power >= desired_power) %>%
  group_by(effect_size) %>%
  summarize(min_n = min(n))

## Print results ##
print(power_results)
print(sample_size_for_power)

## Print the time taken for the power analysis ##
print(power_analysis_time)

## Plot Power Curves ##
ggplot(power_results, aes(x = n, y = power, color = as.factor(effect_size))) +
  geom_line() +
  labs(
    title = "Power Analysis",
    x = "Sample Size",
    y = "Power",
    color = "Effect Size"
  ) +
  theme_minimal()

```


```{r}
## Seed ##
set.seed(42) # the hitchhiker's guide to the galaxy

## Data generation function ##
generate_data <- function(
                          n_control,            # Control
                          n_intervention_1,     # Perceptual learning
                          n_intervention_2,     # Tai-Chi
                          pre_mean,             # Pre-treatment mean for all groups
                          effect_control,       # Effect size for control
                          effect_intervention_1,# Effect size for intervention 1
                          effect_intervention_2,# Effect size for intervention 2
                          sd_pre,               # Standard deviation for pre-treatment
                          sd_change             # Standard deviation for change scores
                          ) {
  
  # Simulate pre-treatment scores
  pre_control <- rnorm(n_control, mean = pre_mean, sd = sd_pre)
  pre_intervention_1 <- rnorm(n_intervention_1, mean = pre_mean, sd = sd_pre)
  pre_intervention_2 <- rnorm(n_intervention_2, mean = pre_mean, sd = sd_pre)
  
  # Simulate post-treatment scores by adding the effect size multiplied by the SD
  post_control <- pre_control + rnorm(n_control, mean = effect_control * sd_pre, sd = sd_change)
  post_intervention_1 <- pre_intervention_1 + rnorm(n_intervention_1, mean = effect_intervention_1 * sd_pre, sd = sd_change)
  post_intervention_2 <- pre_intervention_2 + rnorm(n_intervention_2, mean = effect_intervention_2 * sd_pre, sd = sd_change)
  
  data_control <- tibble(
                         condition = "control",
                         pre_score = pre_control,
                         post_score = post_control
                         )
  
  data_intervention_1 <- tibble(
                                condition = "PL",
                                pre_score = pre_intervention_1,
                                post_score = post_intervention_1
                                )
  
  data_intervention_2 <- tibble(
                                condition = "tai_chi",
                                pre_score = pre_intervention_2,
                                post_score = post_intervention_2
                                )
  
  data <- bind_rows(
                    data_control,
                    data_intervention_1,
                    data_intervention_2
                    )
  
  return(data)
}

## Analysis function ##
calculate_effect_size <- function(data) {
  mean_diff_1 <- mean(data$post_score[data$condition == "PL"]) - mean(data$post_score[data$condition == "control"])
  mean_diff_2 <- mean(data$post_score[data$condition == "tai_chi"]) - mean(data$post_score[data$condition == "control"])
  
  pooled_sd <- sqrt(((sd(data$post_score[data$condition == "PL"])^2) + (sd(data$post_score[data$condition == "control"])^2) + 
                     (sd(data$post_score[data$condition == "tai_chi"])^2)) / 3)
  
  cohen_d_1 <- mean_diff_1 / pooled_sd
  cohen_d_2 <- mean_diff_2 / pooled_sd
  
  tibble(cohen_d_1 = cohen_d_1, cohen_d_2 = cohen_d_2)
}

## Define experiment parameters ##
experiment_parameters_grid <- expand_grid(
                                          n_control = 30,
                                          n_intervention_1 = 30,
                                          n_intervention_2 = 30,
                                          pre_mean = 50,              # Pre-treatment mean
                                          effect_control = 0,         # No real effect for control
                                          effect_intervention_1 = 0.5,# Small to moderate effect for intervention 1
                                          effect_intervention_2 = 0.8,# Moderate to large effect for intervention 2
                                          sd_pre = 10,                # Standard deviation for pre-treatment
                                          sd_change = 5,              # Standard deviation for change scores
                                          iteration = 1:10000
                                          )

## Parallel setup ##
num_cores <- detectCores() - 1  # Use one less than the number of available cores
cl <- makeCluster(num_cores)

# Export necessary objects and functions to the cluster
clusterExport(cl, list(
                       "experiment_parameters_grid",
                       "generate_data",
                       "analyse_data"
                       ))

clusterEvalQ(cl, {
              library(dplyr)
              library(tidyr)
              })

## Measure the time for the simulation ##
simulation_time <- system.time({
  ## Run simulation with parallel processing ##
  simulation <- experiment_parameters_grid |>
    mutate(
           generated_data = parLapply(
                                      cl,
                                      1:n(),
                                      function(i) {
                                                   set.seed(42 + i)  # Set a different seed for each worker
                                                   params <- experiment_parameters_grid[i, ]
                                                   generate_data(
                                                                 params$n_control,
                                                                 params$n_intervention_1, 
                                                                 params$n_intervention_2,
                                                                 params$pre_mean,
                                                                 params$effect_control,
                                                                 params$effect_intervention_1,
                                                                 params$effect_intervention_2,
                                                                 params$sd_pre,
                                                                 params$sd_change
                                                                 )
                                                   }
                                      )
           ) |>
    mutate(
           effect_size_results = parLapply(
                                           cl,
                                           generated_data,
                                           calculate_effect_size
                                           )
           ) |>
    mutate(
           cohen_d_1 = map_dbl(
                               effect_size_results,
                               ~ .x$cohen_d_1
                               ),
           cohen_d_2 = map_dbl(
                               effect_size_results,
                               ~ .x$cohen_d_2
                               )
           )
})

## Stop cluster ##
stopCluster(cl)

## Calculate average effect sizes ##
result <- simulation |>
  summarise(
            mean_cohen_d_1 = mean(cohen_d_1),
            mean_cohen_d_2 = mean(cohen_d_2)
            )

## Print the average effect sizes ##
print(result)

## Print the time taken for the simulation ##
print(simulation_time)

```


```{r}

# Combine the effect sizes into one data frame for easier plotting
effect_size_data <- simulation |>
  select(cohen_d_1, cohen_d_2) |>
  pivot_longer(
    cols = everything(),
    names_to = "Intervention",
    values_to = "Cohen_d"
  ) |>
  mutate(Intervention = recode(Intervention, 
                               cohen_d_1 = "Perceptual Learning (PL)",
                               cohen_d_2 = "Tai-Chi"))

# Histogram of Cohen's d
ggplot(effect_size_data, aes(x = Cohen_d, fill = Intervention)) +
  geom_histogram(alpha = 0.6, position = "identity", bins = 30) +
  labs(title = "Distribution of Cohen's d for Each Intervention",
       x = "Cohen's d",
       y = "Frequency") +
  theme_minimal()

```


```{r}

result_table <- tibble(
  Intervention = c("Perceptual Learning (PL)", "Tai-Chi"),
  Mean_Cohen_d = c(result$mean_cohen_d_1, result$mean_cohen_d_2)
)

# Display the table
result_table%>% 
  kable(col.names = c(
                      "Condition",
                      "Mean cohen's d"
                      )
        ) |>
  kable_classic(
                full_width = FALSE,
                html_font = "Arial"
                ) |>
  kable_styling(bootstrap_options = c(
                                      "striped",
                                      "hover",
                                      "condensed"
                                      ),
                full_width = F
                )

```

```{r}

# Bar plot of average Cohen's d
ggplot(result_table, aes(x = Intervention, y = Mean_Cohen_d, fill = Intervention)) +
  geom_bar(stat = "identity") +
  labs(title = "Average Effect Size (Cohen's d) by Intervention",
       x = "Intervention",
       y = "Mean Cohen's d") +
  theme_minimal()


# Combine the effect sizes into one data frame for easier plotting
effect_size_data <- simulation |>
  select(cohen_d_1, cohen_d_2) |>
  pivot_longer(
    cols = everything(),
    names_to = "Intervention",
    values_to = "Cohen_d"
  ) |>
  mutate(Intervention = recode(Intervention, 
                               cohen_d_1 = "Perceptual Learning (PL)",
                               cohen_d_2 = "Tai-Chi"))

# Histogram of Cohen's d
ggplot(effect_size_data, aes(x = Cohen_d, fill = Intervention)) +
  geom_histogram(alpha = 0.6, position = "identity", bins = 30) +
  labs(title = "Distribution of Cohen's d for Each Intervention",
       x = "Cohen's d",
       y = "Frequency") +
  theme_minimal()


```


# Session info

```{r}
sessionInfo()
```
