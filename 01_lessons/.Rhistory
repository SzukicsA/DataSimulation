# disable scientific notation
options(scipen=999)
# dependencies
library(tidyverse)
library(scales)
library(knitr)
library(kableExtra)
library(janitor)
# install.packages("devtools")
# library(devtools)
# devtools::install_github("ianhussey/simulateR")
library(simulateR) # available from github - uncomment the above lines to install
# set seed
set.seed(42)
rct_simulation_plot <- function(n_per_group, m1, m2, sd1, sd2){
require(ggplot2)
# simulate data
simulated_scores <- data_frame(score = c(rnorm(n = n_per_group, mean = m1, sd = sd1),
rnorm(n = n_per_group, mean = m2, sd = sd2)),
condition = c(rep("Control", n_per_group),
rep("Intervention", n_per_group)))
# plot
p <-
ggplot(data.frame(simulated_scores), aes(score)) +
geom_histogram(binwidth = 1, boundary = -0.5, position = "identity", fill = "darkcyan") +
scale_x_continuous(breaks = breaks_pretty(),
name = "Depression score (BDI-II)") +
scale_fill_viridis_d(begin = 0.3, end = 0.7, option = "G") +
ylab("Count") +
theme_linedraw() +
theme(panel.grid.minor = element_blank()
#,
#text = element_text(family = "Courier New")
) +
facet_wrap(~ condition, ncol = 1)
return(p)
}
rct_simulation_plot(n_per_group = 40, m1 = 33, m2 = 29, sd1 = 10, sd2 = 12)
rct_simulation_plot(n_per_group = 1000000, m1 = 33, m2 = 29, sd1 = 10, sd2 = 12)
runif(n = 1,
min = 1,
max = 10) |>
round_half_up(0)
set.seed(43) # set the starting seed value for generating the random numbers
runif(n = 1,
min = 1,
max = 10) |>
round_half_up(0)
set.seed(43) # set it again to the same value starting seed value for generating the random numbers
runif(n = 1,
min = 1,
max = 10) |>
round_half_up(0)
set.seed(43) # set the starting seed value for generating the random numbers
runif(n = 1,
min = 1,
max = 10) |>
round_half_up(0)
runif(n = 1,
min = 1,
max = 10) |>
round_half_up(0)
set.seed(43) # set the starting seed value for generating the random numbers
# generate both of the above numbers in one function call
runif(n = 2, # generate two numbers rather than one
min = 1,
max = 10) |>
round_half_up(0)
#$$\mu$$ -> mu symbol $$\sigma$$ -> sigma symbol
rnorm(n = 100,
mean = 7.52,
sd = 3.18)
simulated_scores <- rnorm(n = 1000000, # note that we need lots and lots of data to get a precise estimate
mean = 7.52,
sd = 3.18)
mean(simulated_scores) |> round_half_up(2)
sd(simulated_scores) |> round_half_up(2)
simulated_scores <-
rnorm(n = 1000000, # sample n
mean = 0, # population mean (μ or mu)
sd = 1) # population sd (σ or sigma)
dat <- data.frame(simulated_scores = simulated_scores)
ggplot(dat, aes(x = simulated_scores)) +
geom_histogram()
rnorm_histogram(n = 1000000,
mean = 0,
sd = 1)
rnorm_histogram(n = 1000000,
mean = 0,
sd = 1)
rnorm_histogram(n = 1000000,
mean = -2,
sd = 1,
fill = "darkcyan")
rnorm_histogram(n = 1000000,
mean = 0,
sd = 1)
rnorm_histogram(n = 1000000,
mean = 0,
sd = 2,
fill = "darkcyan")
rnorm_histogram(n = 1000000,
mean = 0,
sd = 1)
rnorm_histogram(n = 1000000,
mean = -2,
sd = 2,
fill = "darkcyan")
rnorm_histogram(n = 1000000,
mean = 0,
sd = 1)
rnorm_histogram(n = 1000,
mean = 0,
sd = 1,
fill = "darkcyan")
rnorm_histogram(n = 100,
mean = 0,
sd = 1,
fill = "darkorange")
set.seed(238)
rnorm_histogram(n = 50,
mean = 0,
sd = 1,
fill = "darkgreen")
rnorm_histogram(n = 50,
mean = 0,
sd = 1,
fill = "darkblue")
rnorm_histogram(n = 50,
mean = 0,
sd = 1,
fill = "darkred")
# define the parameters
n_samples <- 100 # number of samples in each simulation
mu <- 2.25       # population mean
sigma <- 1       # population standard deviation
# make an annotated histogram
rnorm_histogram(n = n_samples,
mean = mu,
sd = sigma)
simulated_scores <-
rnorm(n = n_samples,
mean = mu,
sd = sigma)
mean(simulated_scores) |> round_half_up(2)
#sd(simulated_scores) |> round_half_up(2)
for(i in 1:10){
# generate data sampled from a normal population using rnorm
simulated_scores <-
rnorm(n = n_samples,
mean = mu,
sd = sigma)
# compute the mean for this simulation and print it
mean(simulated_scores) |>
print()
}
for(whatever_varible_name_you_want in 1:10){ # only this line differs from the previous chunk
# generate data sampled from a normal population using rnorm
simulated_scores <-
rnorm(n = n_samples,
mean = mu,
sd = sigma)
# compute the mean for this simulation and print it
mean(simulated_scores) |>
print()
}
n_iterations <- 10
for(i in 1:n_iterations){
# generate data sampled from a normal population using rnorm
simulated_scores <-
rnorm(n = n_samples,
mean = mu,
sd = sigma)
# compute the mean for this simulation and print it
mean(simulated_scores) |>
print()
}
n_iterations <- 10
# print
n_iterations
results <- numeric(n_iterations)
# print
results
# number of elements in the vector == n_iterations
length(results)
results[1] <- 5
# print
results
results[5] <- 4
# print
results
for(i in 1:n_iterations){
results[i] <- 7
}
# print
results
for(i in 1:n_iterations){
results[i] <- i * 2
}
# print
results
# the only difference compared to the first example at the top of the "Multiple iterations of a given simulation" section
# is the resulting means are saved to the results vector.
# But it requires you to think about the loop in a deeper way, and the variable value of i and what its implications are.
for(i in 1:n_iterations){
# generate data sampled from a normal population using rnorm
simulated_scores <-
rnorm(n = n_samples,
mean = mu,
sd = sigma)
# compute the mean for this simulation and store it
# in the `i`th element of the results vector
results[i] <- mean(simulated_scores)
}
# print
results
# calculate the mean of means
mean(results) |> round_half_up(2)
for(i in 1:n_iterations){
# generate data sampled from a normal population using rnorm
simulated_scores <-
rnorm(n = n_samples,
mean = mu,
sd = sigma)
# compute the mean for this simulation and assign it to results
# results[i] <- mean(simulated_scores) # <- the old code from the previous chunk
results <- mean(simulated_scores) # only this line differs from the previous chunk. No element of the vector is specified.
}
# print
results
# we increase the number of iterations to simulate a longer run of experiments
n_iterations <- 10000
for(i in 1:n_iterations){
# generate data sampled from a normal population using rnorm
simulated_scores <-
rnorm(n = n_samples,
mean = mu,
sd = sigma)
# compute the mean for this simulation and store it
# in the `i`th element of the results vector
results[i] <- mean(simulated_scores)
}
# calculate the mean of means
mean(results) |>
round_half_up(2)
# check that the mean of sample means is equal to the population mean (mu)
mean(results) |> round_half_up(2) == mu
set.seed(42)
# new values
n_samples <- 100
n_iterations <- 10000
mu <- -2.84
sigma <- 5.10
# create two new results vectors
results_means <- numeric(n_iterations)
results_sds <- numeric(n_iterations)
for(i in 1:n_iterations){
# generate data sampled from a normal population using rnorm
simulated_scores <-
rnorm(n = n_samples,
mean = mu,
sd = sigma)
# compute the mean for this simulation and store it
# in the `i`th element of each results vector
results_means[i] <- mean(simulated_scores)
results_sds[i] <- sd(simulated_scores)
}
# compute the mean of means
mean(results_means) |> round_half_up(2)
# check that the mean of sample means is equal to the population mean (mu)
mean(results_means) |> round_half_up(2) == mu
# compute the mean of SDs
mean(results_sds) |> round_half_up(2)
# check that the mean of sample SDs is equal to the population SD (sigma)
mean(results_sds) |> round_half_up(2) == sigma
# set seed for reproducibility
set.seed(42)
# simulation parameters
n_control      <- 50
n_intervention <- 50
mu_control      <- 0 # both mu values are zero: population effect is null.
mu_intervention <- 0
sigma_control      <- 1
sigma_intervention <- 1
n_iterations <- 10000
# create results vector
results_ps <- numeric(n_iterations)
# for loop used to repeat this many times
for(i in 1:n_iterations){
# data generation
data_control      <- rnorm(n = n_control,      mean = mu_control,      sd = sigma_control)
data_intervention <- rnorm(n = n_intervention, mean = mu_intervention, sd = sigma_intervention)
# data analysis
p <- t.test(x = data_control,
y = data_intervention,
var.equal = TRUE,
alternative = "two.sided")$p.value
results_ps[i] <- p
}
# summarise results across iterations
# compute the false positive rate (proportion of significant p values when population effect is null)
mean(results_ps < .05) |> round_half_up(2)
sessionInfo()
knitr::opts_chunk$set(message = FALSE,
warning = FALSE)
library(tidyverse)
library(janitor)
library(knitr)
library(kableExtra)
library(faux)
data_bdi <- read_csv("data/bdi_data.csv")
knitr::opts_chunk$set(message = FALSE,
warning = FALSE)
library(tidyverse)
library(janitor)
library(knitr)
library(kableExtra)
library(faux)
data_bdi <- read_csv("data/bdi_data.csv")
knitr::opts_chunk$set(message = FALSE,
warning = FALSE)
library(tidyverse)
library(janitor)
library(knitr)
library(kableExtra)
library(faux)
data_bdi <- read_csv("bdi_data.csv")
set.seed(42)
#set.seed(49)
subset_no_preselection <- data_bdi |>
rename(bdi_pre = bdi_score) |>
# simulate a 'post' score that is 5 points lower than pre
mutate(bdi_post = bdi_pre - 5) |>
#mutate(n = n()) |>
#mutate(bdi_post = bdi_pre + rnorm(n = n, mean = -5, sd = 1)) |>
# sample 100 participants from the real data
slice_sample(n = 100) |>
mutate(recruitment = "General population")
subset_preselection_for_severe <- data_bdi |>
rename(bdi_pre = bdi_score) |>
# simulate a 'post' score that is 5 points lower than pre
mutate(bdi_post = bdi_pre - 5) |>
#mutate(n = n()) |>
#mutate(bdi_post = bdi_pre + rnorm(n = n, mean = -5, sd = 1)) |>
# simulate recruitment into the study requiring a score of 29 or more at pre ("severe" depression according to the BDI-II manual)
filter(bdi_pre >= 29) |>
# sample 100 participants from the pre=selected real data
slice_sample(n = 100) |>
mutate(recruitment = "'Severe' depression")
subset_combined <-
bind_rows(subset_no_preselection,
subset_preselection_for_severe)
# table of results
subset_estimates <- subset_combined |>
group_by(recruitment) |>
summarize(n = n(),
mean_pre = mean(bdi_pre),
mean_post = mean(bdi_post),
sd_pre = sd(bdi_pre),
sd_post = sd(bdi_post)) |>
mutate(mean_diff = mean_post - mean_pre,
cohens_d = (mean_post - mean_pre) / ( (sd_post + sd_pre)/2 )) |>
select(recruitment,
n,
mean_pre, sd_pre, mean_post, sd_post,
mean_diff,
cohens_d)
subset_estimates |>
mutate_if(is.numeric, janitor::round_half_up, digits = 1) |>
kable() |>
kable_classic(full_width = FALSE)
# plot
subset_combined |>
rename(Pre = bdi_pre, Post = bdi_post) |>
pivot_longer(cols = c(Pre, Post),
names_to = "timepoint",
values_to = "bdi_score") |>
mutate(timepoint = fct_relevel(timepoint, "Pre", "Post"),
recruitment = fct_relevel(recruitment, "General population", "'Severe' depression")) |>
ggplot(aes(bdi_score)) +
#geom_vline(xintercept = 29, linetype = "dashed") +
#geom_histogram(boundary = 0, bins = 67) +
geom_histogram(boundary = 0, bins = 21) +
scale_fill_viridis_d(begin = 0.3, end = 0.7) +
theme_linedraw() +
coord_cartesian(xlim = c(-5, 63)) +
facet_grid(timepoint ~ recruitment) +
xlab("BDI-II sum score") +
ylab("Frequency")
set.seed(46)
#set.seed(45)
subset_no_preselection_2 <- data_bdi |>
rename(bdi_pre = bdi_score) |>
# simulate a 'post' score that is 5 points lower than pre
mutate(bdi_post = bdi_pre - 5) |>
#mutate(n = n()) |>
#mutate(bdi_post = bdi_pre + rnorm(n = n, mean = -5, sd = 1)) |>
# sample 100 participants from the real data
slice_sample(n = 100) |>
mutate(recruitment = "General population")
subset_preselection_for_severe_2 <- data_bdi |>
rename(bdi_pre = bdi_score) |>
# simulate a 'post' score that is 3 points lower than pre
mutate(bdi_post = bdi_pre - 3) |>
#mutate(n = n()) |>
#mutate(bdi_post = bdi_pre + rnorm(n = n, mean = -3, sd = 1)) |>
# simulate recruitment into the study requiring a score of 29 or more at pre ("severe" depression according to the BDI-II manual)
filter(bdi_pre >= 29) |>
# sample 100 participants from the pre=selected real data
slice_sample(n = 100) |>
mutate(recruitment = "'Severe' depression")
subset_combined_2 <-
bind_rows(subset_no_preselection_2,
subset_preselection_for_severe_2)
# table of results
subset_estimates_2 <- subset_combined_2 |>
group_by(recruitment) |>
summarize(n = n(),
mean_pre = mean(bdi_pre),
mean_post = mean(bdi_post),
sd_pre = sd(bdi_pre),
sd_post = sd(bdi_post)) |>
mutate(mean_diff = mean_post - mean_pre,
cohens_d = (mean_post - mean_pre) / ( (sd_post + sd_pre)/2 )) |>
select(recruitment,
n,
mean_pre, sd_pre, mean_post, sd_post,
mean_diff,
cohens_d)
subset_estimates_2 |>
mutate_if(is.numeric, janitor::round_half_up, digits = 1) |>
kable() |>
kable_classic(full_width = FALSE)
# plot
subset_combined_2 |>
rename(Pre = bdi_pre, Post = bdi_post) |>
pivot_longer(cols = c(Pre, Post),
names_to = "timepoint",
values_to = "bdi_score") |>
mutate(timepoint = fct_relevel(timepoint, "Pre", "Post"),
recruitment = fct_relevel(recruitment, "General population", "'Severe' depression")) |>
ggplot(aes(bdi_score)) +
#geom_vline(xintercept = 29, linetype = "dashed") +
#geom_histogram(boundary = 0, bins = 67) +
geom_histogram(boundary = 0, bins = 21) +
scale_fill_viridis_d(begin = 0.3, end = 0.7) +
theme_linedraw() +
coord_cartesian(xlim = c(-5, 63)) +
facet_grid(timepoint ~ recruitment) +
xlab("BDI-II sum score") +
ylab("Frequency")
# Set seed for reproducibility
set.seed(42)
# Parameters
n <- 10000  # number of observations
rho <- 0.6  # correlation between x and y
# Generate correlated data using the faux package
simulated_data <- rnorm_multi(n = n,
mu = c(0, 0),
sd = c(1, 1),
r = matrix(c(1, rho,
rho, 1), nrow = 2),
varnames = c("x", "y"))
# Calculate correlation in full data
full_correlation <- cor(simulated_data$x, simulated_data$y)
cat("Correlation in full data:", janitor::round_half_up(full_correlation, digits = 2), "\n")
# Introduce range restriction (e.g., keep only x > -0.5 and x < 0.5)
simulated_data_range_restricted <- simulated_data |>
filter(x > qnorm(0.75)) # top 25% of a normal population corresponds to SD > qnorm(0.75), ie 0.6744898
# Calculate correlation in restricted data
restricted_correlation <- cor(simulated_data_range_restricted$x, simulated_data_range_restricted$y)
cat("Correlation in restricted data:", janitor::round_half_up(restricted_correlation, digits = 2), "\n")
# Plot full data with correlation annotation
ggplot(simulated_data, aes(x = x, y = y)) +
geom_point(alpha = 0.4) +
#geom_smooth(method = "lm", se = FALSE, color = "blue") +
ggtitle("Correlation in Full Data") +
theme_linedraw() +
annotate("text", x = -2, y = 2, label = paste("r =", round(full_correlation, 2)),
hjust = 0.5, vjust = 0.5, size = 6, color = "blue") +
coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3))
# Plot restricted data with correlation annotation
ggplot(simulated_data_range_restricted, aes(x = x, y = y)) +
geom_point(alpha = 0.4) +
#geom_smooth(method = "lm", se = FALSE, color = "red") +
ggtitle("Correlation in Range Restricted Data") +
theme_linedraw() +
annotate("text", x = -2, y = 2, label = paste("r =", round(restricted_correlation, 2)),
hjust = 0.5, vjust = 0.5, size = 6, color = "red") +
coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3))
# Calculate the variance ratios as an estimate of the range restriction factor
variance_ratio <- var(simulated_data_range_restricted$x) / var(simulated_data$x)
# Deattenuate the observed correlation
corrected_correlation <- restricted_correlation / sqrt(variance_ratio)
# Output results
cat("Observed Correlation (Restricted):", janitor::round_half_up(restricted_correlation, 2), "\n")
cat("Variance Ratio (Range Restriction Factor):", janitor::round_half_up(variance_ratio, 2), "\n")
cat("Corrected Correlation (Deattenuated):", janitor::round_half_up(corrected_correlation, 2), "\n")
knitr::opts_chunk$set(message = FALSE,
warning = FALSE)
options(scipen=999)
# Dependencies
# N.B.: to ensure full computational reproducibility, R version 4.3.3 should be used.
library(groundhog)
groundhog_day = "2024-04-07"
packages = c("effectsize","faux","janitor",
"rstatix", "effsize",
"psych", "MBESS", "lsr",
"metafor", "esc", "esci",
"dplyr", "tidyr", "tibble",
"forcats", "ggplot2", "stringr")
groundhog.library(packages, groundhog_day)
